---
id: QC-024.03
title: Comparison Commands
status: To Do
assignee: []
created_date: '2026-01-29 16:44'
labels:
  - application
  - collaboration
  - P1
dependencies: []
parent_task_id: QC-024
---

## Description

<!-- SECTION:DESCRIPTION:BEGIN -->
Commands: CompareCoders (calculate Kappa), GetAgreementMatrix, ExportReliabilityReport. Support AI vs human comparison.
<!-- SECTION:DESCRIPTION:END -->

## Acceptance Criteria
<!-- AC:BEGIN -->
### Controller Commands (Imperative Shell)
- [ ] #1 `compare_coders(coder_a, coder_b, filters) -> Result[CoderComparisonReport, Error]`
- [ ] #2 `get_agreement_matrix(coder_a, coder_b) -> AgreementMatrix`
- [ ] #3 `export_reliability_report(comparison, path) -> Result[Path, ExportError]`

### AI vs Human Comparison
- [ ] #4 `compare_ai_with_human(human_coder_id) -> CoderComparisonReport`
- [ ] #5 Special case for evaluating AI coder performance
- [ ] #6 Identifies where AI agrees/disagrees with human

### Deriver Functions (Pure)
- [ ] #7 `derive_comparison(coder_a_segments, coder_b_segments) -> ComparisonData`
- [ ] #8 `derive_kappa(matrix) -> KappaResult` - delegates to KappaCalculator

### Comparison Filters
- [ ] #9 `ComparisonFilter`: code_ids, source_ids, date_range
- [ ] #10 Filter comparison to specific subset

### Event Publishing
- [ ] #11 `ComparisonCalculated` event: coder_a, coder_b, kappa_score

### Export
- [ ] #12 Export to XLSX with formatted Kappa table
- [ ] #13 Include per-code breakdown
<!-- AC:END -->
