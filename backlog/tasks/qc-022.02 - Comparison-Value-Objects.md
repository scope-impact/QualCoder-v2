---
id: QC-022.02
title: Comparison Value Objects
status: To Do
assignee: []
created_date: '2026-01-29 16:43'
labels:
  - domain
  - collaboration
  - P1
dependencies: []
parent_task_id: QC-022
---

## Description

<!-- SECTION:DESCRIPTION:BEGIN -->
Value objects: KappaResult, AgreementMatrix, ComparisonPair, ReliabilityTrend. Support inter-rater reliability calculations.
<!-- SECTION:DESCRIPTION:END -->

## Acceptance Criteria
<!-- AC:BEGIN -->
### KappaResult Value Object (frozen dataclass)
- [ ] #1 `KappaResult`: kappa_score (float), interpretation (str), agreement_count, disagreement_count
- [ ] #2 `interpretation()` method: "Poor"|"Fair"|"Moderate"|"Good"|"Excellent"
- [ ] #3 Immutable - result of pure calculation

### AgreementMatrix Value Object (frozen dataclass)
- [ ] #4 `AgreementMatrix`: matrix (Dict[CodeId, Dict[CodeId, int]]), coder_a, coder_b
- [ ] #5 Matrix cell [a][b] = count where coder_a used code a and coder_b used code b
- [ ] #6 Methods: `get_agreement(code)`, `get_total_agreements()`

### ComparisonPair Value Object
- [ ] #7 `ComparisonPair`: segment_id, coder_a_code, coder_b_code, is_agreement (bool)
- [ ] #8 List of pairs shows detailed comparison

### ReliabilityTrend Value Object
- [ ] #9 `ReliabilityTrend`: data_points (List[KappaDataPoint]), trend_direction
- [ ] #10 `KappaDataPoint`: date, kappa_score, sample_size
- [ ] #11 Tracks Kappa over time for progress monitoring

### Pure Functions
- [ ] #12 `calculate_kappa(matrix: AgreementMatrix) -> KappaResult` - Cohen's Kappa
- [ ] #13 `interpret_kappa(score: float) -> str` - Landis & Koch interpretation
<!-- AC:END -->
